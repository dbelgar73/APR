{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR5_lyXPSqtY"
      },
      "source": [
        "# P4 Regularización, Normalización y Aumentado de datos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2uah_gcSqta",
        "outputId": "db1f65a3-fba2-430d-a7f6-86c4a6a68274",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "training set (60000, 28, 28)\n",
            "test set (10000, 28, 28)\n",
            "training set (48000, 784)\n",
            "val set (12000, 784)\n"
          ]
        }
      ],
      "source": [
        "## Importar y normalizar datos\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print('training set', x_train.shape)\n",
        "print('test set', x_test.shape)\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize [0..255]-->[0..1]\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "num_classes=10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "print('training set', x_train.shape)\n",
        "print('val set', x_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwipfRRzSqte"
      },
      "source": [
        "## Modelo base\n",
        " Partiremos de una topología base e iremos añadiendo diferentes estrategias de regularización para mejorar el rendimiento del modelo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-PAZwGeSqtf",
        "outputId": "4a8d91d3-2a83-4511-d3ae-2529f9f23dfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.3477 - accuracy: 0.8989\n",
            "Epoch 1: val_accuracy improved from -inf to 0.94850, saving model to best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\david\\anaconda3\\envs\\vsc\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "375/375 [==============================] - 41s 85ms/step - loss: 0.3477 - accuracy: 0.8989 - val_loss: 0.1809 - val_accuracy: 0.9485 - lr: 0.0250\n",
            "Epoch 2/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.1332 - accuracy: 0.9613\n",
            "Epoch 2: val_accuracy improved from 0.94850 to 0.96133, saving model to best_model.h5\n",
            "375/375 [==============================] - 30s 81ms/step - loss: 0.1332 - accuracy: 0.9613 - val_loss: 0.1284 - val_accuracy: 0.9613 - lr: 0.0250\n",
            "Epoch 3/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0897 - accuracy: 0.9739\n",
            "Epoch 3: val_accuracy improved from 0.96133 to 0.96958, saving model to best_model.h5\n",
            "375/375 [==============================] - 30s 81ms/step - loss: 0.0897 - accuracy: 0.9739 - val_loss: 0.1015 - val_accuracy: 0.9696 - lr: 0.0250\n",
            "Epoch 4/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9811\n",
            "Epoch 4: val_accuracy improved from 0.96958 to 0.97525, saving model to best_model.h5\n",
            "375/375 [==============================] - 31s 84ms/step - loss: 0.0647 - accuracy: 0.9811 - val_loss: 0.0838 - val_accuracy: 0.9753 - lr: 0.0250\n",
            "Epoch 5/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9869\n",
            "Epoch 5: val_accuracy improved from 0.97525 to 0.97650, saving model to best_model.h5\n",
            "375/375 [==============================] - 25s 66ms/step - loss: 0.0469 - accuracy: 0.9869 - val_loss: 0.0807 - val_accuracy: 0.9765 - lr: 0.0250\n",
            "Epoch 6/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9900\n",
            "Epoch 6: val_accuracy improved from 0.97650 to 0.97700, saving model to best_model.h5\n",
            "375/375 [==============================] - 34s 90ms/step - loss: 0.0357 - accuracy: 0.9900 - val_loss: 0.0783 - val_accuracy: 0.9770 - lr: 0.0250\n",
            "Epoch 7/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.0269 - accuracy: 0.9929\n",
            "Epoch 7: val_accuracy improved from 0.97700 to 0.97892, saving model to best_model.h5\n",
            "375/375 [==============================] - 32s 85ms/step - loss: 0.0269 - accuracy: 0.9929 - val_loss: 0.0701 - val_accuracy: 0.9789 - lr: 0.0250\n",
            "Epoch 8/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9955\n",
            "Epoch 8: val_accuracy did not improve from 0.97892\n",
            "375/375 [==============================] - 31s 81ms/step - loss: 0.0191 - accuracy: 0.9955 - val_loss: 0.0732 - val_accuracy: 0.9775 - lr: 0.0250\n",
            "Epoch 9/25\n",
            "374/375 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9970\n",
            "Epoch 9: val_accuracy improved from 0.97892 to 0.97967, saving model to best_model.h5\n",
            "375/375 [==============================] - 34s 91ms/step - loss: 0.0146 - accuracy: 0.9970 - val_loss: 0.0676 - val_accuracy: 0.9797 - lr: 0.0250\n",
            "Epoch 10/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9982\n",
            "Epoch 10: val_accuracy improved from 0.97967 to 0.98058, saving model to best_model.h5\n",
            "375/375 [==============================] - 36s 97ms/step - loss: 0.0110 - accuracy: 0.9982 - val_loss: 0.0652 - val_accuracy: 0.9806 - lr: 0.0250\n",
            "Epoch 11/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9991\n",
            "Epoch 11: val_accuracy improved from 0.98058 to 0.98150, saving model to best_model.h5\n",
            "375/375 [==============================] - 36s 97ms/step - loss: 0.0078 - accuracy: 0.9991 - val_loss: 0.0652 - val_accuracy: 0.9815 - lr: 0.0250\n",
            "Epoch 12/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 0.9992\n",
            "Epoch 12: val_accuracy improved from 0.98150 to 0.98225, saving model to best_model.h5\n",
            "375/375 [==============================] - 28s 76ms/step - loss: 0.0060 - accuracy: 0.9992 - val_loss: 0.0655 - val_accuracy: 0.9822 - lr: 0.0250\n",
            "Epoch 13/25\n",
            "375/375 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 0.9998"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mh:\\Documentos\\.UPV\\4t\\APR\\Practicas\\P4\\p4.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Documentos/.UPV/4t/APR/Practicas/P4/p4.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m epochs\u001b[39m=\u001b[39m\u001b[39m25\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Documentos/.UPV/4t/APR/Practicas/P4/p4.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/h%3A/Documentos/.UPV/4t/APR/Practicas/P4/p4.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, y_train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Documentos/.UPV/4t/APR/Practicas/P4/p4.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m                     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Documentos/.UPV/4t/APR/Practicas/P4/p4.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m                     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Documentos/.UPV/4t/APR/Practicas/P4/p4.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m                     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Documentos/.UPV/4t/APR/Practicas/P4/p4.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m(x_val, y_val),\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Documentos/.UPV/4t/APR/Practicas/P4/p4.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                     callbacks\u001b[39m=\u001b[39;49m[reduce_lr,checkpoint])  \n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Documentos/.UPV/4t/APR/Practicas/P4/p4.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m## Cargar el mejor modelo y evaluarlo con el test set\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Documentos/.UPV/4t/APR/Practicas/P4/p4.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mload_model(\u001b[39m'\u001b[39m\u001b[39mbest_model.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\vsc\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\vsc\\lib\\site-packages\\keras\\src\\engine\\training.py:1832\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1816\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1817\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[0;32m   1818\u001b[0m         x\u001b[39m=\u001b[39mval_x,\n\u001b[0;32m   1819\u001b[0m         y\u001b[39m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1830\u001b[0m         pss_evaluation_shards\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pss_evaluation_shards,\n\u001b[0;32m   1831\u001b[0m     )\n\u001b[1;32m-> 1832\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m   1833\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[0;32m   1834\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[0;32m   1835\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[0;32m   1836\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[0;32m   1837\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   1838\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1839\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1840\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1841\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1842\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1843\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1844\u001b[0m )\n\u001b[0;32m   1845\u001b[0m val_logs \u001b[39m=\u001b[39m {\n\u001b[0;32m   1846\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1847\u001b[0m }\n\u001b[0;32m   1848\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
            "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\vsc\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\vsc\\lib\\site-packages\\keras\\src\\engine\\training.py:2272\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2268\u001b[0m             \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   2269\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m   2270\u001b[0m             ):\n\u001b[0;32m   2271\u001b[0m                 callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 2272\u001b[0m                 logs \u001b[39m=\u001b[39m test_function_runner\u001b[39m.\u001b[39;49mrun_step(\n\u001b[0;32m   2273\u001b[0m                     dataset_or_iterator,\n\u001b[0;32m   2274\u001b[0m                     data_handler,\n\u001b[0;32m   2275\u001b[0m                     step,\n\u001b[0;32m   2276\u001b[0m                     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pss_evaluation_shards,\n\u001b[0;32m   2277\u001b[0m                 )\n\u001b[0;32m   2279\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2280\u001b[0m \u001b[39m# Override with model metrics instead of last step logs\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\vsc\\lib\\site-packages\\keras\\src\\engine\\training.py:4079\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[1;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[0;32m   4078\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(\u001b[39mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[1;32m-> 4079\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_function(dataset_or_iterator)\n\u001b[0;32m   4080\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   4081\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\vsc\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\vsc\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\vsc\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:876\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    874\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 876\u001b[0m results \u001b[39m=\u001b[39m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    877\u001b[0m     args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_config\n\u001b[0;32m    878\u001b[0m )\n\u001b[0;32m    879\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[0;32m    880\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    881\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\vsc\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\vsc\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mflat_call(args)\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\vsc\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\vsc\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    253\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    254\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    255\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    256\u001b[0m     )\n\u001b[0;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\vsc\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1480\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1485\u001b[0m   )\n\u001b[0;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
            "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\vsc\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input(784))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "opt=SGD(learning_rate=0.025, momentum=0.9)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=opt,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                patience=2, min_lr=0.00001)\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "epochs=25\n",
        "batch_size=128\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[reduce_lr,checkpoint])\n",
        "\n",
        "## Cargar el mejor modelo y evaluarlo con el test set\n",
        "model = keras.models.load_model('best_model.h5')\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPG2scfDSqth"
      },
      "source": [
        "## Regularización l2 (o l1)\n",
        "\n",
        "La regularización l2 consiste en añadir a la función de coste una penalización proporcional a la norma l2 de los pesos del modelo. De esta forma, se penaliza a los pesos que tengan un valor alto, forzando a que los pesos tengan valores pequeños. Esto se conoce como regularización l2. También podríamos hacer lo mismo con regularización l1 o con ambas (lo que se conoce como *Elastic net*)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGwy8hSjSqti"
      },
      "outputs": [],
      "source": [
        "## Teniendo en cuenta el modelo base añade regularización L2 a las capas densas\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input(784))\n",
        "model.add(Dense(1024, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dense(1024, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "opt=SGD(learning_rate=0.025, momentum=0.9)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=opt,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                patience=2, min_lr=0.00001)\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "epochs=25\n",
        "batch_size=128\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[reduce_lr,checkpoint])\n",
        "\n",
        "## Cargar el mejor modelo y evaluarlo con el test set\n",
        "model = keras.models.load_model('best_model.h5')\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6zoTXLVSqtl"
      },
      "source": [
        "## Dropout\n",
        "\n",
        "El dropout es una técnica de regularización que consiste en eliminar aleatoriamente un porcentaje de las neuronas de la red durante el entrenamiento. De esta forma, se evita que la red se sobreajuste a los datos de entrenamiento y se mejora la generalización del modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0joXY7YSqtl"
      },
      "outputs": [],
      "source": [
        "## Teniendo en cuenta el modelo base añade regularización de tipo dropout a las capas densas\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input(784))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "opt=SGD(learning_rate=0.025, momentum=0.9)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=opt,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                patience=2, min_lr=0.00001)\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "epochs=25\n",
        "batch_size=128\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[reduce_lr,checkpoint])\n",
        "\n",
        "## Cargar el mejor modelo y evaluarlo con el test set\n",
        "model = keras.models.load_model('best_model.h5')\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSo9j9orSqtn"
      },
      "source": [
        "## Normalización BatchNorm\n",
        "\n",
        "La normalización BatchNorm consiste en normalizar la salida de una capa de la red neuronal para que tenga media 0 y varianza 1. De esta forma, se consigue que la red neuronal pueda entrenarse más rápido y que sea más robusta a cambios en los pesos de las capas anteriores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4rAW2KzSqto"
      },
      "outputs": [],
      "source": [
        "## Teniendo en cuenta el modelo base añade normalización BatchNorm\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input, Dropout, BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input(784))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "opt=SGD(learning_rate=0.025, momentum=0.9)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=opt,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                patience=2, min_lr=0.00001)\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "epochs=25\n",
        "batch_size=128\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[reduce_lr,checkpoint])\n",
        "\n",
        "## Cargar el mejor modelo y evaluarlo con el test set\n",
        "model = keras.models.load_model('best_model.h5')\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3DMxUMRSqtp"
      },
      "source": [
        "## Aumentado de datos\n",
        "\n",
        "El aumentado de datos consiste en generar nuevos datos de entrenamiento a partir de los datos de entrenamiento originales. De esta forma, se consigue que el modelo sea más robusto y que se generalice mejor a datos que no ha visto durante el entrenamiento.\n",
        "\n",
        "En nuestro caso para los dígitos de la MNIST vamos a realizar un aumento de datos de la siguiente forma:\n",
        "\n",
        "- Rotación aleatoria de la imagen entre -30 y 30 grados.\n",
        "- Traslación aleatoria de la imagen entre -3 y 3 píxeles en horizontal y vertical.\n",
        "- Escalado aleatorio de la imagen entre 0.8 y 1.2.\n",
        "- Inversión aleatoria de la imagen en horizontal y vertical. **NO!!!**\n",
        "\n",
        "El aumentado de datos se ejecuta en CPU y ralentiza el entrenamiento.\n",
        "\n",
        "Normalmente además, se necesitarán más epochs para entrenar el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPv5kQkWSqtq"
      },
      "outputs": [],
      "source": [
        "## Implementamos en el ejemplo base el aumentado de datos\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input, Dropout, BatchNormalization,Reshape\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "## Importante: ImageDataGenerator espera una imagen con 3 canales, necesitamos hacer reshape\n",
        "x_train = x_train.reshape(48000, 28, 28, 1)\n",
        "x_val = x_val.reshape(12000, 28, 28, 1)\n",
        "x_test = x_test.reshape(10000, 28, 28, 1)\n",
        "\n",
        "## Ajustamos el generador de datos\n",
        "datagen.fit(x_train)\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input, Dropout, BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input((28,28,1)))\n",
        "model.add(Reshape((784,)))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "opt=SGD(learning_rate=0.025, momentum=0.9)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=opt,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=2, min_lr=0.00001)\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "epochs=25\n",
        "batch_size=128\n",
        "## Entrenamos con el generador de datos en lugar de con el dataset\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[reduce_lr,checkpoint])\n",
        "\n",
        "## Cargar el mejor modelo y evaluarlo con el test set\n",
        "model = keras.models.load_model('best_model.h5')\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nMJmVehSqtr"
      },
      "source": [
        "## Ejercicio:\n",
        "\n",
        "Probar todas las técnicas presentadas para obtener un acierto en **test > 99%**.\n",
        "\n",
        "Se aconseja no malgastar datos de entrenamiento y por lo tanto emplear todo el training set para el entrenamiento. No emplear conjunto de validación y emplear el test set al final para calcular el acierto.\n",
        "\n",
        "A modo de \"trampa\" podríamos ejecutar el fit con los datos de test en validation_data para así monitorizar si llegamos a ese 99%\n",
        "\n",
        "validation_data=(x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input, BatchNormalization, Reshape\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input, Dropout, BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "##########################################################################Importar datos#########################################\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "## Importante: ImageDataGenerator espera una imagen con 3 canales, necesitamos hacer reshape\n",
        "x_train=x_train.reshape(60000, 28, 28, 1)\n",
        "x_test=x_test.reshape(10000, 28, 28, 1)\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "print(\"Training set: \", x_train.shape)\n",
        "print(\"Test set: \", x_test.shape)\n",
        "\n",
        "####################################################################### Normalizar ################################################\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "# convert class vectors to binary class matrices\n",
        "num_classes=10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "################################################################# Aumentado de datos  ##############################################\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "## Ajustamos el generador de datos\n",
        "datagen.fit(x_train)\n",
        "##################################################################  MODELO  ##############################################\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input((28,28,1)))\n",
        "model.add(Reshape((784,)))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())#Normalizacion Batch,\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "#optimizador\n",
        "opt=Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=opt,\n",
        "            metrics=['accuracy'])\n",
        "#callbacks Checkpoint y EarlyStop\n",
        "#callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3) # con early stopping no llega a optimizar suficiente!!!\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=2, min_lr=0.00001)\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "#Numero de epochs\n",
        "epochs = 100\n",
        "batch_size = 128\n",
        "#Entrenar el modelo, (no se usará conjunto de validación) Utilizar trampa: los datos de test utilizarlos como test del training\n",
        "## Entrenamos con el generador de datos en lugar de con el dataset\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[reduce_lr, checkpoint])\n",
        "#Probar el modelo con los test\n",
        "## Cargar el mejor modelo y evaluarlo con el test set\n",
        "model = keras.models.load_model('best_model.h5')\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gvjsAD4-RO92",
        "outputId": "ceda9424-1234-4287-fe22-3180e9227a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "Training set:  (60000, 28, 28, 1)\n",
            "Test set:  (10000, 28, 28, 1)\n",
            "Epoch 1/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.6481 - accuracy: 0.8155\n",
            "Epoch 1: val_accuracy improved from -inf to 0.95160, saving model to best_model.h5\n",
            "469/469 [==============================] - 23s 44ms/step - loss: 0.6473 - accuracy: 0.8157 - val_loss: 0.1580 - val_accuracy: 0.9516 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.3066 - accuracy: 0.9050\n",
            "Epoch 2: val_accuracy did not improve from 0.95160\n",
            "469/469 [==============================] - 20s 44ms/step - loss: 0.3066 - accuracy: 0.9050 - val_loss: 0.1694 - val_accuracy: 0.9445 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.2484 - accuracy: 0.9224\n",
            "Epoch 3: val_accuracy improved from 0.95160 to 0.96600, saving model to best_model.h5\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.2481 - accuracy: 0.9225 - val_loss: 0.1159 - val_accuracy: 0.9660 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.2182 - accuracy: 0.9319\n",
            "Epoch 4: val_accuracy improved from 0.96600 to 0.97240, saving model to best_model.h5\n",
            "469/469 [==============================] - 21s 44ms/step - loss: 0.2183 - accuracy: 0.9319 - val_loss: 0.0887 - val_accuracy: 0.9724 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.1958 - accuracy: 0.9393\n",
            "Epoch 5: val_accuracy improved from 0.97240 to 0.97380, saving model to best_model.h5\n",
            "469/469 [==============================] - 23s 48ms/step - loss: 0.1959 - accuracy: 0.9392 - val_loss: 0.0849 - val_accuracy: 0.9738 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.9417\n",
            "Epoch 6: val_accuracy did not improve from 0.97380\n",
            "469/469 [==============================] - 21s 44ms/step - loss: 0.1875 - accuracy: 0.9417 - val_loss: 0.0938 - val_accuracy: 0.9725 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.1768 - accuracy: 0.9449\n",
            "Epoch 7: val_accuracy did not improve from 0.97380\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.1767 - accuracy: 0.9450 - val_loss: 0.0926 - val_accuracy: 0.9712 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.9609\n",
            "Epoch 8: val_accuracy improved from 0.97380 to 0.98410, saving model to best_model.h5\n",
            "469/469 [==============================] - 22s 46ms/step - loss: 0.1246 - accuracy: 0.9609 - val_loss: 0.0500 - val_accuracy: 0.9841 - lr: 0.0020\n",
            "Epoch 9/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.1057 - accuracy: 0.9664\n",
            "Epoch 9: val_accuracy improved from 0.98410 to 0.98450, saving model to best_model.h5\n",
            "469/469 [==============================] - 22s 46ms/step - loss: 0.1057 - accuracy: 0.9664 - val_loss: 0.0464 - val_accuracy: 0.9845 - lr: 0.0020\n",
            "Epoch 10/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9677\n",
            "Epoch 10: val_accuracy did not improve from 0.98450\n",
            "469/469 [==============================] - 19s 42ms/step - loss: 0.1029 - accuracy: 0.9677 - val_loss: 0.0470 - val_accuracy: 0.9837 - lr: 0.0020\n",
            "Epoch 11/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0953 - accuracy: 0.9695\n",
            "Epoch 11: val_accuracy did not improve from 0.98450\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0952 - accuracy: 0.9695 - val_loss: 0.0476 - val_accuracy: 0.9839 - lr: 0.0020\n",
            "Epoch 12/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0888 - accuracy: 0.9721\n",
            "Epoch 12: val_accuracy improved from 0.98450 to 0.98770, saving model to best_model.h5\n",
            "469/469 [==============================] - 20s 43ms/step - loss: 0.0887 - accuracy: 0.9721 - val_loss: 0.0371 - val_accuracy: 0.9877 - lr: 4.0000e-04\n",
            "Epoch 13/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9735\n",
            "Epoch 13: val_accuracy improved from 0.98770 to 0.98790, saving model to best_model.h5\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0823 - accuracy: 0.9735 - val_loss: 0.0367 - val_accuracy: 0.9879 - lr: 4.0000e-04\n",
            "Epoch 14/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0817 - accuracy: 0.9740\n",
            "Epoch 14: val_accuracy did not improve from 0.98790\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.0817 - accuracy: 0.9740 - val_loss: 0.0381 - val_accuracy: 0.9869 - lr: 4.0000e-04\n",
            "Epoch 15/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9757\n",
            "Epoch 15: val_accuracy did not improve from 0.98790\n",
            "469/469 [==============================] - 21s 46ms/step - loss: 0.0775 - accuracy: 0.9757 - val_loss: 0.0366 - val_accuracy: 0.9872 - lr: 4.0000e-04\n",
            "Epoch 16/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.9757\n",
            "Epoch 16: val_accuracy did not improve from 0.98790\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.0793 - accuracy: 0.9757 - val_loss: 0.0364 - val_accuracy: 0.9875 - lr: 4.0000e-04\n",
            "Epoch 17/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9760\n",
            "Epoch 17: val_accuracy improved from 0.98790 to 0.98820, saving model to best_model.h5\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0794 - accuracy: 0.9760 - val_loss: 0.0355 - val_accuracy: 0.9882 - lr: 4.0000e-04\n",
            "Epoch 18/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9758\n",
            "Epoch 18: val_accuracy improved from 0.98820 to 0.98850, saving model to best_model.h5\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0772 - accuracy: 0.9758 - val_loss: 0.0350 - val_accuracy: 0.9885 - lr: 4.0000e-04\n",
            "Epoch 19/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9760\n",
            "Epoch 19: val_accuracy improved from 0.98850 to 0.98890, saving model to best_model.h5\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.0738 - accuracy: 0.9760 - val_loss: 0.0334 - val_accuracy: 0.9889 - lr: 4.0000e-04\n",
            "Epoch 20/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9775\n",
            "Epoch 20: val_accuracy did not improve from 0.98890\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.0715 - accuracy: 0.9775 - val_loss: 0.0356 - val_accuracy: 0.9878 - lr: 4.0000e-04\n",
            "Epoch 21/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9770\n",
            "Epoch 21: val_accuracy did not improve from 0.98890\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.0736 - accuracy: 0.9770 - val_loss: 0.0336 - val_accuracy: 0.9887 - lr: 4.0000e-04\n",
            "Epoch 22/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0701 - accuracy: 0.9779\n",
            "Epoch 22: val_accuracy improved from 0.98890 to 0.98950, saving model to best_model.h5\n",
            "469/469 [==============================] - 22s 46ms/step - loss: 0.0702 - accuracy: 0.9779 - val_loss: 0.0325 - val_accuracy: 0.9895 - lr: 8.0000e-05\n",
            "Epoch 23/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9770\n",
            "Epoch 23: val_accuracy did not improve from 0.98950\n",
            "469/469 [==============================] - 21s 44ms/step - loss: 0.0726 - accuracy: 0.9770 - val_loss: 0.0326 - val_accuracy: 0.9894 - lr: 8.0000e-05\n",
            "Epoch 24/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0684 - accuracy: 0.9782\n",
            "Epoch 24: val_accuracy improved from 0.98950 to 0.98960, saving model to best_model.h5\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.0685 - accuracy: 0.9782 - val_loss: 0.0322 - val_accuracy: 0.9896 - lr: 8.0000e-05\n",
            "Epoch 25/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0663 - accuracy: 0.9784\n",
            "Epoch 25: val_accuracy improved from 0.98960 to 0.98970, saving model to best_model.h5\n",
            "469/469 [==============================] - 21s 46ms/step - loss: 0.0663 - accuracy: 0.9784 - val_loss: 0.0324 - val_accuracy: 0.9897 - lr: 8.0000e-05\n",
            "Epoch 26/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9790\n",
            "Epoch 26: val_accuracy did not improve from 0.98970\n",
            "469/469 [==============================] - 20s 43ms/step - loss: 0.0679 - accuracy: 0.9790 - val_loss: 0.0321 - val_accuracy: 0.9897 - lr: 8.0000e-05\n",
            "Epoch 27/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9781\n",
            "Epoch 27: val_accuracy did not improve from 0.98970\n",
            "469/469 [==============================] - 22s 46ms/step - loss: 0.0702 - accuracy: 0.9781 - val_loss: 0.0325 - val_accuracy: 0.9896 - lr: 1.6000e-05\n",
            "Epoch 28/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.9784\n",
            "Epoch 28: val_accuracy did not improve from 0.98970\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.0694 - accuracy: 0.9784 - val_loss: 0.0323 - val_accuracy: 0.9892 - lr: 1.6000e-05\n",
            "Epoch 29/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9778\n",
            "Epoch 29: val_accuracy did not improve from 0.98970\n",
            "469/469 [==============================] - 22s 46ms/step - loss: 0.0680 - accuracy: 0.9778 - val_loss: 0.0323 - val_accuracy: 0.9897 - lr: 1.0000e-05\n",
            "Epoch 30/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9791\n",
            "Epoch 30: val_accuracy did not improve from 0.98970\n",
            "469/469 [==============================] - 20s 43ms/step - loss: 0.0662 - accuracy: 0.9791 - val_loss: 0.0324 - val_accuracy: 0.9894 - lr: 1.0000e-05\n",
            "Epoch 31/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9784\n",
            "Epoch 31: val_accuracy did not improve from 0.98970\n",
            "469/469 [==============================] - 22s 46ms/step - loss: 0.0681 - accuracy: 0.9784 - val_loss: 0.0318 - val_accuracy: 0.9896 - lr: 1.0000e-05\n",
            "Epoch 32/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9786\n",
            "Epoch 32: val_accuracy did not improve from 0.98970\n",
            "469/469 [==============================] - 20s 43ms/step - loss: 0.0669 - accuracy: 0.9786 - val_loss: 0.0321 - val_accuracy: 0.9896 - lr: 1.0000e-05\n",
            "Epoch 33/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9785\n",
            "Epoch 33: val_accuracy did not improve from 0.98970\n",
            "469/469 [==============================] - 21s 44ms/step - loss: 0.0691 - accuracy: 0.9785 - val_loss: 0.0323 - val_accuracy: 0.9894 - lr: 1.0000e-05\n",
            "Epoch 34/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9785\n",
            "Epoch 34: val_accuracy did not improve from 0.98970\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0686 - accuracy: 0.9785 - val_loss: 0.0318 - val_accuracy: 0.9897 - lr: 1.0000e-05\n",
            "Epoch 35/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9786\n",
            "Epoch 35: val_accuracy did not improve from 0.98970\n",
            "469/469 [==============================] - 20s 43ms/step - loss: 0.0665 - accuracy: 0.9786 - val_loss: 0.0320 - val_accuracy: 0.9897 - lr: 1.0000e-05\n",
            "Epoch 36/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0702 - accuracy: 0.9786\n",
            "Epoch 36: val_accuracy improved from 0.98970 to 0.98980, saving model to best_model.h5\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0701 - accuracy: 0.9786 - val_loss: 0.0321 - val_accuracy: 0.9898 - lr: 1.0000e-05\n",
            "Epoch 37/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0680 - accuracy: 0.9787\n",
            "Epoch 37: val_accuracy did not improve from 0.98980\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.0680 - accuracy: 0.9787 - val_loss: 0.0323 - val_accuracy: 0.9896 - lr: 1.0000e-05\n",
            "Epoch 38/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9785\n",
            "Epoch 38: val_accuracy did not improve from 0.98980\n",
            "469/469 [==============================] - 21s 44ms/step - loss: 0.0664 - accuracy: 0.9785 - val_loss: 0.0321 - val_accuracy: 0.9898 - lr: 1.0000e-05\n",
            "Epoch 39/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9783\n",
            "Epoch 39: val_accuracy improved from 0.98980 to 0.99000, saving model to best_model.h5\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.0676 - accuracy: 0.9783 - val_loss: 0.0319 - val_accuracy: 0.9900 - lr: 1.0000e-05\n",
            "Epoch 40/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0677 - accuracy: 0.9787\n",
            "Epoch 40: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 20s 43ms/step - loss: 0.0677 - accuracy: 0.9787 - val_loss: 0.0322 - val_accuracy: 0.9898 - lr: 1.0000e-05\n",
            "Epoch 41/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9790\n",
            "Epoch 41: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.0679 - accuracy: 0.9790 - val_loss: 0.0322 - val_accuracy: 0.9896 - lr: 1.0000e-05\n",
            "Epoch 42/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9783\n",
            "Epoch 42: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 22s 46ms/step - loss: 0.0680 - accuracy: 0.9783 - val_loss: 0.0323 - val_accuracy: 0.9894 - lr: 1.0000e-05\n",
            "Epoch 43/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0674 - accuracy: 0.9789\n",
            "Epoch 43: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 20s 43ms/step - loss: 0.0674 - accuracy: 0.9788 - val_loss: 0.0323 - val_accuracy: 0.9891 - lr: 1.0000e-05\n",
            "Epoch 44/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9784\n",
            "Epoch 44: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0673 - accuracy: 0.9784 - val_loss: 0.0323 - val_accuracy: 0.9892 - lr: 1.0000e-05\n",
            "Epoch 45/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9778\n",
            "Epoch 45: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0683 - accuracy: 0.9778 - val_loss: 0.0323 - val_accuracy: 0.9895 - lr: 1.0000e-05\n",
            "Epoch 46/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9785\n",
            "Epoch 46: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 21s 46ms/step - loss: 0.0688 - accuracy: 0.9785 - val_loss: 0.0324 - val_accuracy: 0.9892 - lr: 1.0000e-05\n",
            "Epoch 47/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9783\n",
            "Epoch 47: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.0689 - accuracy: 0.9783 - val_loss: 0.0320 - val_accuracy: 0.9894 - lr: 1.0000e-05\n",
            "Epoch 48/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9779\n",
            "Epoch 48: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 22s 46ms/step - loss: 0.0696 - accuracy: 0.9779 - val_loss: 0.0321 - val_accuracy: 0.9892 - lr: 1.0000e-05\n",
            "Epoch 49/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0680 - accuracy: 0.9789\n",
            "Epoch 49: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 21s 44ms/step - loss: 0.0679 - accuracy: 0.9789 - val_loss: 0.0322 - val_accuracy: 0.9892 - lr: 1.0000e-05\n",
            "Epoch 50/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9786\n",
            "Epoch 50: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 20s 43ms/step - loss: 0.0670 - accuracy: 0.9786 - val_loss: 0.0322 - val_accuracy: 0.9897 - lr: 1.0000e-05\n",
            "Epoch 51/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0693 - accuracy: 0.9783\n",
            "Epoch 51: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 23s 49ms/step - loss: 0.0694 - accuracy: 0.9783 - val_loss: 0.0322 - val_accuracy: 0.9892 - lr: 1.0000e-05\n",
            "Epoch 52/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.9784\n",
            "Epoch 52: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 23s 48ms/step - loss: 0.0678 - accuracy: 0.9784 - val_loss: 0.0321 - val_accuracy: 0.9893 - lr: 1.0000e-05\n",
            "Epoch 53/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9788\n",
            "Epoch 53: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 21s 44ms/step - loss: 0.0655 - accuracy: 0.9788 - val_loss: 0.0317 - val_accuracy: 0.9894 - lr: 1.0000e-05\n",
            "Epoch 54/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0684 - accuracy: 0.9784\n",
            "Epoch 54: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 22s 48ms/step - loss: 0.0684 - accuracy: 0.9784 - val_loss: 0.0316 - val_accuracy: 0.9894 - lr: 1.0000e-05\n",
            "Epoch 55/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.9793\n",
            "Epoch 55: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 23s 49ms/step - loss: 0.0656 - accuracy: 0.9793 - val_loss: 0.0318 - val_accuracy: 0.9895 - lr: 1.0000e-05\n",
            "Epoch 56/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9794\n",
            "Epoch 56: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 20s 43ms/step - loss: 0.0660 - accuracy: 0.9795 - val_loss: 0.0318 - val_accuracy: 0.9894 - lr: 1.0000e-05\n",
            "Epoch 57/100\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9786\n",
            "Epoch 57: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 23s 50ms/step - loss: 0.0675 - accuracy: 0.9786 - val_loss: 0.0321 - val_accuracy: 0.9894 - lr: 1.0000e-05\n",
            "Epoch 58/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0689 - accuracy: 0.9786\n",
            "Epoch 58: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.0689 - accuracy: 0.9786 - val_loss: 0.0320 - val_accuracy: 0.9896 - lr: 1.0000e-05\n",
            "Epoch 59/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0656 - accuracy: 0.9792\n",
            "Epoch 59: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 21s 44ms/step - loss: 0.0656 - accuracy: 0.9792 - val_loss: 0.0320 - val_accuracy: 0.9897 - lr: 1.0000e-05\n",
            "Epoch 60/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0656 - accuracy: 0.9791\n",
            "Epoch 60: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 23s 49ms/step - loss: 0.0658 - accuracy: 0.9791 - val_loss: 0.0321 - val_accuracy: 0.9894 - lr: 1.0000e-05\n",
            "Epoch 61/100\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.0672 - accuracy: 0.9786\n",
            "Epoch 61: val_accuracy did not improve from 0.99000\n",
            "469/469 [==============================] - 23s 48ms/step - loss: 0.0673 - accuracy: 0.9785 - val_loss: 0.0321 - val_accuracy: 0.9894 - lr: 1.0000e-05\n",
            "Epoch 62/100\n",
            "333/469 [====================>.........] - ETA: 5s - loss: 0.0666 - accuracy: 0.9781"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e19a2f0adeec>\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m#Entrenar el modelo, (no se usará conjunto de validación) Utilizar trampa: los datos de test utilizarlos como test del training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m## Entrenamos con el generador de datos en lugar de con el dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n\u001b[0m\u001b[1;32m     71\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1781\u001b[0m                         ):\n\u001b[1;32m   1782\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    868\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Solución\n",
        "\n",
        "\n",
        "# Importar y normalizar datos\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 28, 28, 1)\n",
        "x_test = x_test.reshape(10000, 28, 28, 1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "print('training set', x_train.shape)\n",
        "print('test set', x_test.shape)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize [0..255]-->[0..1]\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "num_classes=10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input, BatchNormalization, Reshape\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input, Dropout, BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input((28,28,1)))\n",
        "model.add(Reshape((784,)))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "opt=Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=opt,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=2, min_lr=0.00001)\n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "epochs=50\n",
        "batch_size=128\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[reduce_lr,checkpoint])\n",
        "\n",
        "## Cargar el mejor modelo y evaluarlo con el test set\n",
        "model = keras.models.load_model('best_model.h5')\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4UtjRwQdAMY",
        "outputId": "d28b1938-1919-4398-c252-b53d7ce26893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set (60000, 28, 28, 1)\n",
            "test set (10000, 28, 28, 1)\n",
            "Epoch 1/50\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}